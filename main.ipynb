{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc01510-630f-42c8-9112-4da50a57ab6a",
   "metadata": {
    "panel-layout": {
     "height": 60.3125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# <span style = \"color:rebeccapurple\">Introduction to scikit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc2be4-c689-451a-99eb-bcdd9ef6c8f5",
   "metadata": {
    "panel-layout": {
     "height": 50.09375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## <span style = \"color:darkorchid\">What is scikit-learn?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1e398-a0d4-4253-a250-7af6dd620528",
   "metadata": {
    "panel-layout": {
     "height": 133,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "`scikit-learn` is a user-friendly python library that helps you implement a variety of machine learning algorithms with just a few lines of code. You do not need to be an expert in machine learning to use `scikit-learn`, however, you do need to learn some concepts to use it correctly.\n",
    "\n",
    "Today, we will learn a bit of both. We will alternate between coding and concepts so you can be successful in your future machine learning endeavors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85353d77-3bc8-4cfa-867f-1adc24765050",
   "metadata": {
    "panel-layout": {
     "height": 50.09375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## <span style = \"color:darkorchid\">What do I assume you know?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fbd52-47ae-4dbf-8e81-8e656029e1e2",
   "metadata": {
    "panel-layout": {
     "height": 178,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "This workshop is an introduction to `scikit-learn`, not to programming. Hence, we will assume you have some familiarity with the following:\n",
    "\n",
    "#### Python preliminaries\n",
    "<ul>\n",
    "    <li>Basic Python structures (lists, dictionaries, tuples) and control flow (loops, conditionals, etc.)</li>\n",
    "    <li>Numpy arrays</li>\n",
    "    <li>Pandas dataframes</li>\n",
    "    <li>Basics of matplotlib</li>\n",
    "    <li>Basics of classes and objects (we will do a quick review)</li>\n",
    "</ul>\n",
    "\n",
    "We will have a quick review of python structures, arrays, and dataframes, but we cannot linger much on them. We will use `matplotlib` and `seaborn` for plotting, but you do not really need to know, at this stage, all the specifics for how those work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346de29-28ba-4ad3-b44d-a51c4b7a29af",
   "metadata": {},
   "source": [
    "#### Machine learning preliminaries\n",
    "The reason you came to a `scikit-learn` workshop is because you want to implement machine learning in your projects. We cannot be comprehensive in such a short course, so I will assume you have <b>some familiarity with machine learning</b>, at least what it is and what it tries to do. If you don't, it is OK, we will still review some of those concepts as we proceed, but we may go a bit faster, so make sure you ask plenty of questions, and you are encouraged to review them after the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8264f1a-4f03-4d0f-a6ac-d5c2d2106ff5",
   "metadata": {},
   "source": [
    "# <span style = \"color:rebeccapurple\">Python Review</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bdeb4-85d1-4d8f-aa60-f2c31c0b8ffe",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\">Data Structures in Python</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed3504-aa8e-42be-92b1-54618f23ee55",
   "metadata": {},
   "source": [
    "We will make some use of lists, dictionaries and tuples, and of pandas dataframes. You don't need to be an expert on these, and we can't dwell much on them, but I want to quickly remind you what they are and how they look. If you don't fully understand their behavior don't worry, it is not crucial for today's workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1ab94-6160-47fb-bcc0-31f90e20636b",
   "metadata": {},
   "source": [
    "#### Base python structures\n",
    "Base python has three basic structures you should strive to be acquainted with:\n",
    "<ul>\n",
    "    <li>lists</li>\n",
    "    <li>dictionaries</li>\n",
    "    <li>tuples</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f6419-7e42-4ec7-8bee-4b23922d9042",
   "metadata": {},
   "source": [
    "<b>Lists</b>\n",
    "\n",
    "Lists are ordered collections of objects. You can create empty lists, increase the size of lists, and obtain specific elements through indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ab2b6-d2d0-4006-a0bb-36437faa7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists\n",
    "my_list = []\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf128971-702e-4b4a-851a-b8c841ba791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-empty lists\n",
    "my_list = [1, 2, \"hello\"]\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf19a3d-1c63-41b4-9901-2f66eac7ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an element to a list\n",
    "my_list.append(\"good bye\")\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9050b0-a9bf-4b38-9e24-1824b3f7e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over a list\n",
    "for i in my_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894aa57-b279-43ce-9ba2-e8eaa8792864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing a list\n",
    "print(my_list[0])\n",
    "print(my_list[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732110ec-5ec4-4f3d-9ff8-35fc8b51abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting an element in a list:\n",
    "my_list[3] = \"hello again\"\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca9fdc-1b01-4edf-9e53-37c655b1cf7a",
   "metadata": {},
   "source": [
    "<b>Dictionaries</b>\n",
    "Dictionaries are also collections of objects, but instead of being indexed by order, as in the list case, they have a *key*, which uniquely identifies the *value* of your object. These are called key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd028f58-8711-43e1-a30b-2fcf134ba4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty dictionary\n",
    "my_dict = {}\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01485a-dbe3-41e2-961d-ea7e618962b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A non-empty dictionary with key-value pairs formatted as key:value\n",
    "my_dict = {\"data\": [1,2,3],\n",
    "          \"salutation\": \"hello\",\n",
    "          \"inception\": {\"some key\":\"some value\"}}\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb14fa-abed-4f40-9908-5e25cbc533e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355155a-d721-40f5-a0ba-4c02d4210b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict[\"salutation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d119bd-426a-4d45-b4ce-92b24bbc0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict[\"inception\"] # <-- This is a dictionary inside a dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e75c5-d39b-49b5-a763-2847c70c3599",
   "metadata": {},
   "source": [
    "<i>(Optional) Note:</i> We have used only strings as keys. Most of the time this will be the case. It is possible to use other objects as keys, but we won't go into that (if you know about mutability and hashability, only mutable and hashable objects can be keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3fef7-8b49-4299-b138-5d36f46db550",
   "metadata": {},
   "source": [
    "<b> Tuples </b><br>\n",
    "Tuples are ordered collections of objects, almost like lists, BUT, they are immutable. Meaning that you can't change them as you did with lists. You cannot change their elements, add a new element, deete one, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd7f88-6d00-4942-a3de-3566416f8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: TUPLES ::\n",
    "my_tuple = (1,2,\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2768b-4364-4fda-8043-1e70a457a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474eb36b-5b20-480f-a357-f941c9729ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbca29-0961-463a-88a9-1a5ba5d6150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tuple[0] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08339b-4556-499c-ab5a-a9ebc41b6875",
   "metadata": {},
   "source": [
    "You should have gotten an error in the last cell, that's because you can't change elements of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207926c1-60f8-41b7-82d3-609c3d7da40b",
   "metadata": {},
   "source": [
    "#### Numpy arrays\n",
    "\n",
    "Numpy arrays will take the role of vectors and matrices in python. They are ordered collections of objects, like lists, but there is an important constraint: all elements must be of the same type. Furthermore, if your elements are numeric, you can do numeric operations on the arrays, including matrix multiplication.\n",
    "\n",
    "Numpy is a python package that is not imported by default, so we must import it. Chances are you already have numpy installed, so there is no need to install it. We must only import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f22f6-c8c4-4363-aa39-887cf129b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np        # <-- Numpy is usually imported as 'np' (this is convention)\n",
    "\n",
    "# create an array\n",
    "my_array = np.array([1,2,3])\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8badd70f-1bac-4e48-9c7e-58e24735cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index an array\n",
    "print(my_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75a9bd-9d69-459f-b324-a6f19da20994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform numeric computations with an array:\n",
    "print(2 * my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bd547-fbfc-4419-a7df-67e6a8183942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare that with a list:\n",
    "print(2 * [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2c1f4-211f-4b71-a444-a46571cceeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a matrix:\n",
    "my_matrix = np.array([[0,1,0],\n",
    "                     [1,0,0],\n",
    "                     [0,0,1]])\n",
    "my_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d378397-740a-4211-b526-1b2656cc355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can multiply arrays as if they were matrices/vectors with the matmul() method\n",
    "np.matmul(my_matrix, my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def11e2-e9a7-4dd5-8d3f-82b22f3e8dff",
   "metadata": {},
   "source": [
    "#### Pandas dataframes\n",
    "\n",
    "Pandas is another package that is commonly used in python but must be imported. Pandas is mostly used to manipulate datasets, for example by subsetting. The main object in pandas is the dataframe, which is basically a table. Pandas allows you to manipulate these tables easily. When it comes to datasets, the convention is for rows to be the different observations (for example patients) and for columns to be the observed features (for example age, sex, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fab13d-9e9c-45b1-92c7-26e462ff5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # <-- Pandas is usually imported as 'pd' (this is convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bec256-2639-4e28-8df6-0c9bdcccad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from a list of lists\n",
    "pd.DataFrame(data = [[1,2,],[3,4], [5,6,]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87441f-18bd-4c56-b35d-d70091b2459c",
   "metadata": {},
   "source": [
    "Note it automatically created column headings (0,1 in this case) and row indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaff2bb-1116-42c0-9235-74549a3dd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe with column names:\n",
    "pd.DataFrame(data = [[1,2],[3,4], [5,6]], columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d34a05-9435-48cb-85e6-5236d8c536c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from a dictionary:\n",
    "pd.DataFrame(data = {\"Column 1\": [1, 3, 4], \"Column 2\": [2,4,6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a333eec-fda8-4af9-b016-561ffaf37137",
   "metadata": {},
   "source": [
    "Note that with the dictionary each key-value pair is a column. In the case of nested lists, each sublist is a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7f75e-f922-4025-b4cd-4370ec7447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use numpy arrays:\n",
    "my_matrix = np.array([[0,1,0],\n",
    "                     [1,0,0],\n",
    "                     [0,0,1]])\n",
    "\n",
    "df = pd.DataFrame(data = my_matrix, columns = [\"col1\", \"col2\", \"col3\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98dc1f-c896-43db-8141-e8cde61ba424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the columns of a dataframe, you can call the column names as you'd do with dictionaries:\n",
    "df[\"col1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692addaf-e191-4fad-ad6c-bab538c47471",
   "metadata": {},
   "source": [
    "This actually return a pandas *series*, which are basically single columns (the numbers on the left are the indices, not actual values). If you want to return a *dataframe*, which is often necessary, you can use double brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede17560-91e2-48b0-9bf8-d0bcbc73edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"col1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662e5b1-328b-4902-9b40-accb07031c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create new columns also as with dictionaries:\n",
    "df[\"col4\"] = [1,1,1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3864e6-6af6-4607-a249-a8b544e36acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only want to see the first few elements of your dataframe, use the .head() method:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1962e-85e4-447f-b53d-fefcf3b8bca1",
   "metadata": {},
   "source": [
    "(In this case there is no difference bc we only have three rows, but you will see it used belo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9baf504-152a-4a64-9a89-9f8042513b9a",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\">Classes and Objects</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d941a6-5f2f-4331-a3a7-808aaa28a3b4",
   "metadata": {},
   "source": [
    "Python is very versatile. You can write function after function if you wish (as you would in a language like R), however, its strength stems from object oriented programming. `scikit-learn` makes plenty of use of objects and classes, so let's take a quick review at what these are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0af455-b9f5-4e67-9253-9d3e0fe42418",
   "metadata": {},
   "source": [
    "Imagine you are at a high-end restaurant. Let's say you are seeing Gordon Ramsay at work. There is an executive chef, a head chef, several sous-chefs, specialized chefs (for example for raosting, for pastries, etc.), each with a team of specialists (the butcher, the grill chef, the baker, the confectioner, etc.). Here is an image I got from google images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4b5bc-d5e0-4e50-8bbf-5b25848eeec1",
   "metadata": {},
   "source": [
    "<img src = \"images/brigade-de-cuisine-high-speed-learning.png\" width = 400 style=\"display: block; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccf33e-4185-4308-abad-f377b30a2745",
   "metadata": {},
   "source": [
    "Think of each of these as a type or a *class* of chefs. There are important things to note:\n",
    "<ul>\n",
    "    <li>They are all of the generic type <i>chef</i>, but some have extra skills or responsibilities.</li>\n",
    "    <li>You can have several chefs of the same class (like several sous-chefs). However, these are not the same people!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d102fd-8586-4664-a909-61ca3c6e66e6",
   "metadata": {},
   "source": [
    "Well, classes in python are something similar. It is a specified type of entity that has specific skills and attributes. Objects are the realizations of these classes. Each realization is called an *instance*. In python, the skills are called *methods*, these are actions that all instances of a class can perform. They also have *attributes* which variables, possibly unique, that each instance have (like the names of individual chefs).\n",
    "\n",
    "We won't go further over classes but the important principle is this:\n",
    "\n",
    "When you are writing a large python code, do not think of yourself as a homecook that does everything by themselves from scratch, following each step one after another. Instead, **think of yourself as an executive chef**. First you appoint all the chefs working for you, each with predetermined skills and attributes. You think of the overall plan, and then you delegate tasks to the respective chefs (which may, in turn, delegate tasks to their respective chefs and specialists)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb50996-1a78-49c3-b9b9-8cef9975ded3",
   "metadata": {},
   "source": [
    "Indeed, `scikit-learn` is very similar. It has classes of classifiers, of pre-processors, of regression models, etc. Your job will not be to cook absolutely everything by yourself, but to organize things conceptually, hire your appropriate chefs, tell them what to do and trust them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae46dc7-14ce-4b6c-9dbd-390222413da6",
   "metadata": {},
   "source": [
    "#### How to deal with classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc53a57-252a-457d-a9e5-f317d666266c",
   "metadata": {},
   "source": [
    "The following are fake examples because `ChefClass` does not exist. Don't run the cells or you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b1f49-144c-42ed-af65-b86ab1ae2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have already a class, you can create an instance of that class like this:\n",
    "chef = ChefClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b685a-c2ef-4384-a43c-2f68d2187880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can obtain attributes using a period:\n",
    "chef.specialty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe6a70-2028-42fb-bc60-b5bf0d74c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You call a method (ask it to perform a skill) using a period and parenthesis:\n",
    "chef.cook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1eaad8-8045-4a6a-b6ab-f8e3e0988ebe",
   "metadata": {},
   "source": [
    "And on to `scikit-learn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f4dda-7aa3-4f6b-8d90-4ada6b89e82c",
   "metadata": {},
   "source": [
    "# <span style = \"color:rebeccapurple\">Hands-on scikit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafaf7c-0fd0-47e7-9a01-dc14498b5cb6",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\"> Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead6ba8-8bd4-4030-b331-9d926ebff552",
   "metadata": {},
   "source": [
    "First, as with all scripts, let's import all the modules we will use for this workshop. `scikit-learn` is referred to as `sklearn`, pronounced \"S - K - Learn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9cda5-460a-478c-bba9-166488b34dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: IMPORTS ::\n",
    "\n",
    "# Scikit-learn specifics:\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Helper modules\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb29f43-3975-48c4-8dfd-0f8c6957719a",
   "metadata": {
    "panel-layout": {
     "height": 60.3125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## <span style = \"color:darkorchid\"> The adventures of Alice and Bo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f340b2-57b1-4f23-9bc3-165d64216404",
   "metadata": {},
   "source": [
    "Throughout the workshop we will follow the adventures of two researchers: Alice and Bo. I will teach with examples from Alice's adventures. You will work on exercises from Bo's breakthroughs.\n",
    "\n",
    "I asked midjourney to generate images of Alice and Bo on the style of Alice in Wonderland. This is what I got:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201554e-169c-430c-8086-879dad57ec88",
   "metadata": {},
   "source": [
    "<img src = \"images/Alice-and-Bo.png\" width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171ac08-8902-4762-9989-5b610835e8f9",
   "metadata": {},
   "source": [
    "# <span style = \"color:rebeccapurple\"> Part 1 - Load and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48095baf-bd2d-4860-b83b-238479c55e43",
   "metadata": {
    "panel-layout": {
     "height": 60.3125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## <span style = \"color:darkorchid\"> Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6ae84-af62-4e06-b05c-5930ab46feaf",
   "metadata": {},
   "source": [
    "We will obtain data in two ways: through `scikit-learn`, which gets the datasets from their website, and using `pandas' to load our own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7615c51-69c0-4b53-8250-2a5b174c734a",
   "metadata": {},
   "source": [
    "### Toy data with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3101a01-d755-4c90-8037-b094d273ad60",
   "metadata": {
    "panel-layout": {
     "height": 50.09375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "`scikit-learn` contains some toy datasets you can load directly from them. We will be using a couple of those in this tutorial. It requires the `datasets` module, which we already imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71adfd-d119-41cb-88a5-eeae4c38646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df607d06-cd56-459c-bf55-6ceb173843ad",
   "metadata": {},
   "source": [
    "Turns out datasets are dictionary-like objects. We can see what they contain by looking at the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b2772-acb7-4d22-987d-23aeeb25bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of object is diabetes?\n",
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380299e9-a288-41ef-99ea-45cf47587e0d",
   "metadata": {},
   "source": [
    "The `DESCR` key contains a description of the dataset. It is a long string, so use it with the `print()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d627bf-5375-4e9d-b940-6aaf44bb4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf429cd5-67d4-4ab2-8f54-9e86c2899e3c",
   "metadata": {},
   "source": [
    "We can obtain the values either by key name, as in dictionaries, or as if they were class attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9e921-97f9-4df7-b510-4bd4154c636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary syntax:\n",
    "diabetes['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e973e3d-96cc-4416-9b42-1ed9c5db8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute syntax:\n",
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f604d-a4ac-4362-ae99-9394abe62b32",
   "metadata": {},
   "source": [
    "The *data* object will be by default a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2700c6-664d-469e-a078-2f4c27910659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default datasets: arrays\n",
    "diabetes.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6a8a1-66b4-4ef6-bacf-cb3aac1cbfc3",
   "metadata": {},
   "source": [
    "But you can also as for it to be a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f0829-2316-4e07-a6ad-9c44412103a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = datasets.load_diabetes(as_frame = True)\n",
    "diabetes_df.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a6366-08fd-4e12-a1e2-ba3ad5606102",
   "metadata": {},
   "source": [
    "Let's review the `.head()` method for pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b9b14-fcba-401b-a73a-140ba844f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda750e-6fb4-4a31-9135-5e09c1b5c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction target is separate:\n",
    "diabetes_df.target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607be8c7-17ce-4e14-8ee9-229ed06a3ebf",
   "metadata": {},
   "source": [
    "`scikit-learn` has other \"not toy\" datasets you can obtain but which we won't go into. You can find more about them <a href = \"https://scikit-learn.org/stable/datasets.html\" target = \"blank_\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381445b6-84d9-413e-b6ba-04c490a3288c",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a91ac-c816-45dd-bff7-0882f5384e4d",
   "metadata": {},
   "source": [
    "`scikit-learn` has an \"iris\" dataset. Based on how we loaded \"diabetes\", can you guess how you can load \"iris\"?\n",
    "1. Load iris, use the `as_dataframe` argument.\n",
    "2. Show the keys so you know how it is structured.\n",
    "3. Print the description of the dataset\n",
    "4. Show the target names.\n",
    "5. Obtain the feature data and the target.\n",
    "6. Show the first few rows of the feature data.\n",
    "7. Show the full target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254485fb-5ad0-46db-a2c4-567348fdd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffaef92-444a-4abb-b64e-8f60e252d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ff38c-ec65-4b85-8b9a-e57cacfe9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fe7a4-115e-482e-896d-3cfb60dec700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show labels (target names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b64f67-0693-434b-b506-8d39ed5d19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54d36a-46ce-450a-8d73-6540de648d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a407c-0c6f-4a6a-80a2-7be00db099be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows of features (predictive data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb6988-f893-4f58-ab5b-75577b533b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de79fc5-7d21-4711-a8e5-30edf1e430de",
   "metadata": {
    "panel-layout": {
     "height": 44.03125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Loading our own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67df56-bc46-4426-9361-259d74489549",
   "metadata": {},
   "source": [
    "<b>NOTE</b> If you are using Google colab, instead of your jupyter lab. You will need to upload the datasets by hand:\n",
    "1. First option, click on the folder icon on the left, then upload it. The datasets are found in the github repository. Note: if you by accident click out of the folder you were in, and it shows you a weird list of folders, you are looking for the one called \"content\".\n",
    "2. Second option, get them directly from github. For that you will need the `!wget` command, and then paste the \"raw content\" link from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89fd6a9-f5a3-4463-a4ee-293c6940c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For COLAB USERS only: change the variable below to 1\n",
    "get_file_yn = 0\n",
    "if get_file_yn:\n",
    "    !wget https://raw.githubusercontent.com/efren-cc/scikit-learn-workshop/main/data/penguins.csv\n",
    "    !wget https://raw.githubusercontent.com/efren-cc/scikit-learn-workshop/main/data/fish.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d66ea8-7b11-45a0-b9e1-afe33ffa16cc",
   "metadata": {},
   "source": [
    "Let's load the \"penguins\" dataset from our \"data\" folder using pandas. Remember we imported pandas as pd above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d1bdf-e061-4410-838d-711f5a7e5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892055c8-a9d5-41fd-86f9-0a7bbfde3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872428a-fb2c-4e9d-801b-14d7b470a736",
   "metadata": {},
   "source": [
    "<b>Summary:</b> If we have our own .csv data, we can load it using pandas `.read_csv()` method. scikit-learn also has toy datasets we can play with, in our case we used `load_diabetes()` to obtain the diabetes dataset, but others are available. See the full list <a href = \"https://scikit-learn.org/stable/datasets/toy_dataset.html\" target = \"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddf576-dd4c-42fe-90b8-380f80ffca8b",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d27db5-c277-407d-8391-fe7270bce4bc",
   "metadata": {},
   "source": [
    "1. Load the fish dataset from our data folder as a pandas dataframe.\n",
    "2. Visualize the first few rows.\n",
    "3. Make a new dataframe with only the \"weight\" column.\n",
    "4. Drop the weight column from the original dataframe, make sure the change is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cf84f-af4a-4ceb-a051-53f15af65faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fish dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e55c5-3bd7-426e-a0fc-ed97f74b12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcc4d4-0fe2-4089-8b53-0b17808a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new dataframe with only weigth column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9416b8-1a1f-4128-aba3-7e32b476927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new dataframe without weight column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48359de5-eea5-44fd-b148-4950ed66d90b",
   "metadata": {
    "panel-layout": {
     "height": 50.09375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## <span style = \"color:darkorchid\">Preprocessing: Transforming data with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0c414-707a-4bba-be97-522401637ac6",
   "metadata": {},
   "source": [
    "Many machine learning algorithms are based on taking distances among data points. Distances will depend on the scale of each dimension (each measured feature). Hence, if the scales are too different from each other (say one feature is measured in the 1000s, and another in decimals), we will get suboptimal, if not completely disastrous, results. There are many different ways to transform your data, which one you choose will depend on the type of data and the algorithm you are using. Here are three commonly used transformations:\n",
    "<ul>\n",
    "    <li>Standardization</li>\n",
    "    <li>Normalization</li>\n",
    "    <li>Encoding categorical features</li>\n",
    "</ul>\n",
    "\n",
    "`scikit-learn` uses the module `preprocessing` to perform the operations above. We already imported in our imports section. Now let's review these transformations one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8e827-d92f-4dab-acfc-f25d8eeaa778",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732528ea-595b-4f34-8bfc-99d62aa32101",
   "metadata": {},
   "source": [
    "Standardization is a statistics based approach to bring all features to a similar scale. It computes the mean and standard deviation over observations of a feature, and then subtracts the mean from each observation and divides by the standard deviation. This results in mean 0, standard deviation 1 statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb830834-a782-4ae6-83dd-0fa6bc2cdec7",
   "metadata": {},
   "source": [
    "The name *standardization* comes from computing the standard score, or z-score, of your observations. This is done in the following manner:\n",
    "$$\n",
    "z_i = \\frac{x_i - \\hat{\\mu}}{\\hat{\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455ab77-ea9c-4249-ac52-bf36784afa34",
   "metadata": {},
   "source": [
    "Let's try it with one of the penguin columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d4307-7514-42e8-b760-4b1bc6cf69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc12ad8-9ebe-456c-8a29-3ce7b024f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how flipper length looks\n",
    "penguins_df[[\"flipper_length_mm\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1aa59-95ba-41b2-952b-4e2e8f53ce3a",
   "metadata": {},
   "source": [
    "Did you notice I used double brackets? `scikit-learn` prefers dataframes as input, and not series, so even though we'll use only one column, we'll keep it as a dataframe. Similarly, if your input is a numpy array, it must be 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fba1fe-3dc4-4ce6-a368-487dab1e65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard scaler object:\n",
    "z_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit it to the data\n",
    "z_scaler.fit(penguins_df[[\"flipper_length_mm\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126eb5d0-5c5e-45a7-ba0f-55288baf9c71",
   "metadata": {},
   "source": [
    "The fitted scaler now has $\\hat{\\mu}$ and $\\hat{\\sigma}$. We can use the `transform` method to transform our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e05313-01f1-411b-9ed7-465b7dbb6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into z-scores\n",
    "z_flipper_length = z_scaler.transform(penguins_df[[\"flipper_length_mm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f47d80-16c0-4392-89fe-86912e91cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_flipper_length[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3c719-db4b-40e8-9b4a-cb2b53d3f24c",
   "metadata": {},
   "source": [
    "Notice that the output is a numpy array. If you are a Pandas fan you could create a new column in your dataframe with these values, or if you are more adept with numpy you can just keep it as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8549b4-0ab4-4482-be64-3150e929cf30",
   "metadata": {},
   "source": [
    "<b>Review</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c3f39-4ad0-437f-8816-f06f9f75a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock data, let's use two columns this time\n",
    "x = penguins_df[[\"flipper_length_mm\", \"bill_depth_mm\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb54966-b2bd-418d-97de-0139d3e95dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard scaler object\n",
    "z_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit it to the data\n",
    "z_scaler.fit(x)\n",
    "\n",
    "# Transform the data\n",
    "z = z_scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4027224-ab96-4d32-a840-e03fb7f2f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a48686-25d7-4fd4-ab0a-f75085fd13cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8ad1e-f5c3-4fad-bcf9-d54cda0a4c4a",
   "metadata": {},
   "source": [
    "Pick any numerical column(s) from your fish dataframe (except weight) and standardize it/them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb6b30-f0d0-46be-9e13-cf4aaa7d1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll reload the data for you\n",
    "fish_df = pd.read_csv(\"data/fish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f06dc2-15e3-45ca-8671-8774477a74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard scaler object\n",
    "\n",
    "\n",
    "# Fit it to the data\n",
    "\n",
    "\n",
    "# Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846817a-1179-41fb-a3d3-d9e0f25f9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 10 elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095a7f2-0e3b-4981-9898-d98d79a46c4f",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3bb3e-4160-4a2d-97c8-c269ec3d27ab",
   "metadata": {},
   "source": [
    "While standardization is statistics based, normalization is geometry based. That means there is an important assumption: the data point to be normalized is assumed to be a vector in a vector space. If you don't know what this is, let's not worry about it at this point. For now, this means we cannot use it in categorical data.\n",
    "\n",
    "Another important point is that normalization happens over the whole ambient space of a data point. That is, it normalizes per row. Notice that, in contrast, standardization was done with column statistics of the whole sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14876022-b08a-483e-8f87-4d1855ea633b",
   "metadata": {},
   "source": [
    "If you are interested, normalization is the process of transforming a vector so it has unit norm:\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{x}{\\left||x\\right||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a77c9-4b25-4dc4-af66-0f3a8759f96c",
   "metadata": {},
   "source": [
    "Let's try it with the first few rows from diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d143d9-739f-48cf-a131-610a0f43330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows before normalization\n",
    "diabetes.data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05750117-982e-401a-8964-1286c8499dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizer\n",
    "norm_scaler = preprocessing.Normalizer(norm = \"l2\")    # <-- \"l2\" indicates Euclidean norm\n",
    "\n",
    "# Fit to data, we can give a whole matrix or only one row:\n",
    "norm_scaler.fit(diabetes.data[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2c7e3-e5de-4c49-9ee1-b155a7eca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_diab = norm_scaler.transform(diabetes.data[0:4])\n",
    "norm_diab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78219047-ad3e-40bd-97c9-7b534269c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(norm_diab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36178ea7-cf52-4e6a-b7a9-34b5c6ee5f23",
   "metadata": {},
   "source": [
    "Note: normalization actually does not require fitting (you are not computing a sample mean and standard deviation as in standardization). Hence, the fit method above is kind of redundant. However, it is kept for consistency among other transformation methods. There is actually a shortcut function that can be used directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f09519-aba6-4da6-90fa-fd2b86bee6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization using one function:\n",
    "norm_diab = preprocessing.normalize(diabetes.data[0:4], norm = \"l2\")\n",
    "\n",
    "norm_diab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1439597-0817-4632-9d41-86d5b1bec503",
   "metadata": {},
   "source": [
    "<b>Summary</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5624e2-1022-4c03-8c6e-e923e29db573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock data\n",
    "x = diabetes.data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e2777-6256-41b5-afd7-3df275070567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizer\n",
    "norm_scaler = preprocessing.Normalizer(norm = \"l2\")\n",
    "\n",
    "# \"Fit\" to data\n",
    "norm_scaler.fit(x)    # <-- doesn't really do much, but keeps syntax/logic consistent\n",
    "\n",
    "# transform data\n",
    "x_norm = norm_scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e08ee-fead-4b2e-93b8-35f011b71152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use the function shortcut:\n",
    "x_norm = preprocessing.normalize(x, norm = \"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0f130-1680-4a80-860b-165a0607b138",
   "metadata": {},
   "source": [
    "The advantage of creating a Normalizer object is that it can be added to a Pipeline object (we will talk about these soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809bbcb-a2ef-4232-b0be-37a2edf9fe47",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d047b1-a825-4171-872e-7f166c6a024c",
   "metadata": {},
   "source": [
    "Normalize the predictive, numerical rows of the fish dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf5b92-3bb8-42a2-b55a-9a0faaf79b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load the data for you, and drop the columns we don't want\n",
    "fish_df = pd.read_csv(\"data/fish.csv\")\n",
    "fish_df = fish_df.drop(columns = [\"species\", \"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe3b2b-34ad-4e14-81cc-421eb309939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizer\n",
    "\n",
    "\n",
    "# \"Fit\" to data\n",
    "\n",
    "\n",
    "# transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdcb8f-0770-48c9-8bad-4b0d82c540cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 10 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fc143-b521-445c-a134-4212ac31528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the norm of the first row is 1\n",
    "np.linalg.norm(___) # <-- Your first row goes inside these parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d04c0b-e8dd-4f7a-8d13-8073664cf587",
   "metadata": {},
   "source": [
    "### Encoding Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57050fd3-2854-4193-bece-3835a9b01610",
   "metadata": {},
   "source": [
    "Categorical variables don't always have the nice properties of numbers (order, distance, etc.). Therefore, we have to encode them in a way we can deal with them mathematically. There are two main ways of doing this, which depends on the categories having an ordered structure or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3321a1-45e1-4082-8d61-6bc17d8b555e",
   "metadata": {},
   "source": [
    "#### Categorical variables with order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376466d-bb02-4395-971d-b2bee0e66342",
   "metadata": {},
   "source": [
    "When your categorical variables have an order structure, you will do *ordinal* encoding, which means you will map them to the integers. For example, if you have a list of grades: $A$, $B$, $C$, etc. You know the following facts:\n",
    "<ul>\n",
    "    <li>$A > B$</li>\n",
    "    <li>$B > C$</li>\n",
    "    <li>$A > C$</li>\n",
    "</ul>\n",
    "\n",
    "Note that these facts are encoded in the integer numbers ${0, 1, 2, 3, ...}$. So we can map each letter to a number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c24cae-849f-4712-ad02-91fd074c440b",
   "metadata": {},
   "source": [
    "To exemplify this, let's make a mock dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db3204-bd26-45d4-9b4b-b4af7354b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df = pd.DataFrame({\"Grades\":[\"A\", \"A\", \"D\", \"B\", \"C\", \"A\", \"C\"]})\n",
    "grades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59fc36-6139-40ca-a6f8-b573fe79fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the OrdinalEncoder\n",
    "ord_encoder = preprocessing.OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8434686-e8da-4a25-9da4-c1e38e1d15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fit it to our data\n",
    "ord_encoder.fit(grades_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237dfef-dbfa-40df-aeb9-eb260ca974ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transform your data\n",
    "ord_encoder.transform(grades_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46869f14-ea51-422b-8081-ce8a5897ab0e",
   "metadata": {},
   "source": [
    "You can also transform new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83df6d-bd3d-4391-b21b-d4d3dffd583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grades_df = pd.DataFrame({\"Grades\":[\"D\", \"D\", \"B\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19445e0e-97bc-4a3f-b98a-aa6fa3c257ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming new data\n",
    "ord_encoder.transform(new_grades_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e644-7c2b-4e30-ab9b-0079c6f196dc",
   "metadata": {},
   "source": [
    "Notice two things:\n",
    "<ol>\n",
    "    <li>The order was alphabetical ($A$ got mapped to $0$), but maybe you wanted the opposite ($D$ to $0$)</li>\n",
    "    <li>What would happen if we try to transform a category it hasn't seen before?</li>\n",
    "</ol>\n",
    "Let's explore these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998911c7-bf23-4c3b-a183-e7cb579037a2",
   "metadata": {},
   "source": [
    "To my knowledge, there is not nice option to encode in reverse alphabetical order. However, we can provide the categories to encode as an explicit list (a list of lists to be precise, where the $i$th list corresponds to the $i$th column in your dataframe). In this case, the order in which we provide these categories will indicate the order of encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fbda4-37b4-4619-9630-40e3d41ddfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicating explicitly the categories:\n",
    "ord_encoder = preprocessing.OrdinalEncoder(categories = [[\"D\", \"C\", \"B\", \"A\"]])\n",
    "\n",
    "ord_encoder.fit(grades_df)\n",
    "\n",
    "ord_encoder.transform(grades_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be300e1-b619-4107-8450-dc2a0d1d06c3",
   "metadata": {},
   "source": [
    "Now $A$ maps to the higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4f88c-f50f-410b-9c27-fa3523af6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch A and B\n",
    "ord_encoder = preprocessing.OrdinalEncoder(categories = [[\"B\", \"A\", \"C\", \"D\"]])\n",
    "\n",
    "ord_encoder.fit(grades_df)\n",
    "\n",
    "ord_encoder.transform(grades_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8354ba-bbc8-48b4-81ef-83691b67ac96",
   "metadata": {},
   "source": [
    "#### Categorical variables with no order:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931040e-42e0-4547-8863-132e23b38c95",
   "metadata": {},
   "source": [
    "If your variables don't have any order whatsoever, the recommended approach is one-hot encoding. This basically maps each value to a vector whose $i$th element is $1$ if it belongs to category $i$, and $0$ otherwise.\n",
    "\n",
    "For example, if we have \"Aardvark\", \"Babirusa\", and \"Capybara\", for which there is no natural order, the encoding could go like this:\n",
    "<ul>\n",
    "    <li>\"Aardvark\" $\\rightarrow [1,0,0]$</li>\n",
    "    <li>\"Babirusa\" $\\rightarrow [0,1,0]$</li>\n",
    "    <li>\"Capybara\" $\\rightarrow [0,0,1]$</li>\n",
    "</ul>\n",
    "Let's see this in action with the penguins island feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496905ef-58ef-49a1-8476-6a562c310461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the original data looks\n",
    "penguins_df[[\"island\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afd69d-aced-479f-aa3b-514c73102994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a OneHotEncoder\n",
    "oh_encoder = preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073d36b-1b05-4097-890b-b9d8b5ccdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fit it to the data\n",
    "oh_encoder.fit(penguins_df[[\"island\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742f2f3-03a7-4d39-996a-576b5bc67b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transform the data\n",
    "oh_islands = oh_encoder.transform(penguins_df[[\"island\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089744cd-10e1-4d8c-92fb-ff75079ca4a8",
   "metadata": {},
   "source": [
    "Note: since the result will be a matrix with a lot of zeros, scikit-learn actually returns in \"compressed sparse row\" format. Like his:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7b913-a6c8-407d-a984-325b53af4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_islands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323b658-2ae7-4a07-ae40-3bc2e8306fe8",
   "metadata": {},
   "source": [
    "We don't have time to go over this in detail, but, you have two options: 1) You can specify you don't want a sparse output with `sparse_output` argument, or you can easily \"decompress\" it by calling the `toarray()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f20c1-4f5f-4618-a854-bcac8a94b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dense array\n",
    "oh_islands_arr = oh_islands.toarray()\n",
    "oh_islands_arr[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098b8cb-744b-4a0a-b117-8584f1df4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OneHotEncoder with sparse_output as False:\n",
    "preprocessing.OneHotEncoder(sparse_output = False).fit(penguins_df[[\"island\"]]).transform(penguins_df[[\"island\"]])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d870139-8b1f-46eb-a69e-27910f4ac117",
   "metadata": {},
   "source": [
    "How do we know which vector element belongs to which category? Use the `categories_` attribute, the order they appear on will be the order of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cad91-a0f6-4daa-96c7-275bd2c08711",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3469a-87f1-4504-b5f2-330e0bdabfbd",
   "metadata": {},
   "source": [
    "How do we put the data back into our dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d3b7c-a6dd-488a-b99e-8a2f9214b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0d647-6a69-47ed-8a5f-7a2a696b42e8",
   "metadata": {},
   "source": [
    "First, we could just create a new dataframe from the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398b0d2-5b83-4fa2-95ef-bd44556ebfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_islands_df = pd.DataFrame(oh_islands_arr, columns = oh_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ec1c8-a772-4c33-9312-19e08b2d4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_islands_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102153f-197a-452a-b1f7-d450eec9d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df.join(oh_islands_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f123266-361d-408f-b9fa-94fb78cf7069",
   "metadata": {},
   "source": [
    "Alternatively, and presumably easier, we can change the encoder's output format to a pandas dataframe, BUT, if that is the case, we must set `sparse_output` as `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c200db-9843-46c8-b046-2ec96aa35dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new encoder with updated sparse_output argument:\n",
    "oh_encoder = preprocessing.OneHotEncoder(sparse_output = False)\n",
    "\n",
    "# Changing the output format of encoder:\n",
    "oh_encoder.set_output(transform = \"pandas\")\n",
    "\n",
    "oh_encoder.fit(penguins_df[[\"island\"]])\n",
    "\n",
    "oh_islands_df = oh_encoder.transform(penguins_df[[\"island\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd60299-aee6-4650-b8ab-fef73068e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_islands_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b638de7-a9ea-44e9-a60f-a6c8d89bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df.join(oh_islands_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a2946-c5e0-4171-8a53-cc9f6a6941c9",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4bcf0-d1ba-4d3c-97a1-539ae74bde43",
   "metadata": {},
   "source": [
    "The first predictive feature of the fish dataset (the species) is categorical.\n",
    "1. Create a one hot encoder and fit it to this data.\n",
    "2. Transform the data.\n",
    "3. Visualize the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a325cc7-e06a-4e4f-bc3e-8490c4bc04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load the data for you, and select the column we want\n",
    "fish_df = pd.read_csv(\"data/fish.csv\")\n",
    "fish_df = fish_df[[\"species\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72055de-4a9d-402d-bf91-949be0730361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new encoder with updated sparse_output argument:\n",
    "\n",
    "\n",
    "# Changing the output format of encoder:\n",
    "\n",
    "\n",
    "# Fit it to the data\n",
    "\n",
    "\n",
    "# Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fed92-36e0-4064-b789-28f64e530a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the transformed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e04583-2cc9-4251-ac67-b03bc8e5d403",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a330a3-ea7c-45b3-911b-4d97ba193e2e",
   "metadata": {},
   "source": [
    "We learn a few classes from the `preprocessing` module: `StandardScaler`, `Normalizer`, `OrdinalEncoder`, and `OneHotEncoder`. There are many more you will learn in your scikit-learn adventures, but we can't deal with those here. Remember the main recipe for preprocessors:\n",
    "1. Create and instance of the object you need. Usually like this: `preprocessor = SomeClass()`</li>\n",
    "2. Fit the preprocessor, usually like this: `preprocessor.fit(X)`</li>\n",
    "3. Transform your data, which may or may not be the same as the one used for fitting. Usually like this: `preprocessor.transform(X_2)`\n",
    "\n",
    "Keep this recipe in mind, because the models we are about to use follow a similar logic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386518a5-ea1f-4efd-bbaf-59fdef8fe099",
   "metadata": {},
   "source": [
    "# <span style = \"color:rebeccapurple\"> Part 2 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec09eda-5147-4809-9c8e-856525cc663b",
   "metadata": {},
   "source": [
    "OK, it's time to get into machine learning models! Let's start with regression, since it's likely most of you have some familiarity with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf07479-efa8-4b5d-baa6-423b28c9057d",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\"> Alice goes to Antarctica!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5deb4-7f2a-4676-99fc-dddb64e86bfe",
   "metadata": {},
   "source": [
    "Alice is studying penguins in Antarctica, and there is a reported shortage of fish in the area. Alice wants to know if this will be consequential for the penguin populations. To find out, she concludes she can calculate a penguin's consuption rate through its body mass. Hence, if she has their body mass, she can estimate if the penguin population is affected by the fish shortage. Seems straighforward enough...\n",
    "\n",
    "However, weighing the penguins is a difficult, slippery task! On the other hand, Alice reasons that with visual characteristics like fipper lenght, bill dimension, and sex, which are easier to obtain, she can estimate the body mass. She uses a fancy camera equipment and a computer vision software to make these measurements.\n",
    "\n",
    "Her researchers already obtained a small sample of visual features and body mass measurements, which she will use to create a model she can use in the future.\n",
    "\n",
    "These are the penguin species:<br>\n",
    "<img src = \"images/penguins.png\" width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8c97b-53ff-4634-a6e7-38bf226e9d44",
   "metadata": {},
   "source": [
    "### <span style = \"color:darkorange\">Intermezzo - What is regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb874aa3-842b-41ab-8b30-bb6de87e512d",
   "metadata": {},
   "source": [
    "See slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dd17f-d1c1-4a67-a029-74fb52ff2095",
   "metadata": {},
   "source": [
    "### <span style = \"color:teal\">Version 1: The classical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64ba10-00cf-495a-a4c9-7f05b1040ea7",
   "metadata": {},
   "source": [
    "#### Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b65f9f-11d4-48d7-93d9-cf35a0b05684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "penguins_df = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21134537-c003-45ef-9c1a-76dc40fbd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what's in there\n",
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fb407-1340-4134-9c51-39f701744d73",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53470a2-cc01-4ee7-97fd-c41543e86c1d",
   "metadata": {},
   "source": [
    "Our data actually has both the predictive features (visual features) and the target (body mass). So let's put the target in a separate dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973d5e7-ae77-4737-850c-a9780c78732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target from dataframe:\n",
    "penguins_y = penguins_df[[\"body_mass_g\"]]\n",
    "penguins_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cbcfb-c32a-4676-a747-b26810d5bf55",
   "metadata": {},
   "source": [
    "Now we have the option of using all features for preduction or just a few. For simplicity let's constrain ourselves to just the numerical values and sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd1116-81ca-48ed-a075-4581c5c7e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just a few features\n",
    "pred_features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"sex\"]\n",
    "penguins_df[pred_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc2153-0b81-4aac-b13a-e4e952428c43",
   "metadata": {},
   "source": [
    "Note we need to identify which features are categorical, and which numerical. From the four we are using, all are numerical except for sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928748f-93b9-405c-b40d-c2af68c84c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\n",
    "cat_features = [\"sex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad918c-3ed0-4543-898d-7af2a5fe9640",
   "metadata": {},
   "source": [
    "We'll need to deal with these differently. Let's use one-hot encoding for sex and a standard scaler for the numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b97886-7dfd-48d5-8a95-85b97ac1e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with numerical features:\n",
    "sd_scaler = preprocessing.StandardScaler()                     # <-- Create scaler\n",
    "sd_scaler.set_output(transform = \"pandas\")                     # <-- Set output to be in pandas dataframe format\n",
    "sd_scaler.fit(penguins_df[num_features])                       # <-- Fit scaler\n",
    "penguins_X = sd_scaler.transform(penguins_df[num_features])    # <-- Transform data\n",
    "\n",
    "penguins_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0171e-1acf-402a-af57-4532343dab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with categorical features\n",
    "oh_encoder = preprocessing.OneHotEncoder(sparse_output = False)\n",
    "oh_encoder.set_output(transform = \"pandas\")\n",
    "oh_encoder.fit(penguins_df[cat_features])\n",
    "pxx = oh_encoder.transform(penguins_df[cat_features])\n",
    "\n",
    "pxx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd8b4-4cad-4589-8a78-9e3f699e8b3d",
   "metadata": {},
   "source": [
    "Now, there is a little trick I want you to know here. As you can see, everything that is not one category is the other, so there is some redundancy in having the two columns above. Indeed, this may cause problems for some models, like linear regression. We can easily solve this by \"dropping\" one of them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addcb36-29a4-4ffb-a999-01a1bfeed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with categorical features - v2\n",
    "oh_encoder = preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\")\n",
    "oh_encoder.set_output(transform = \"pandas\")\n",
    "oh_encoder.fit(penguins_df[cat_features])\n",
    "pxx = oh_encoder.transform(penguins_df[cat_features])\n",
    "\n",
    "pxx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa72f8-3eb6-4a8b-b03f-482e30014e3d",
   "metadata": {},
   "source": [
    "Let's merge those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabce561-6358-46fb-ba87-91dc9ac7fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X = penguins_X.join(pxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a326c65-9c67-44fd-9495-547450e26832",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220112e-9f61-4fe9-92b2-b6d86d2eb419",
   "metadata": {},
   "source": [
    "#### Introducing the Regression object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58444bbd-542e-4cb0-9358-bd75089d6d47",
   "metadata": {},
   "source": [
    "The logic is as with the preprocessors: create the object, then fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd32583-309d-4eb2-a6fb-8d6adb267700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm will stand for \"linear model\"\n",
    "lm_penguins = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20251411-07f2-47c3-be1f-056fa33525f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the data\n",
    "lm_penguins.fit(X = penguins_X, y = penguins_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d9c6a-9a3e-439c-9fcb-cc4a9afbb966",
   "metadata": {},
   "source": [
    "Great! Now what? Well, we can look at the coefficients like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a7e25-06c5-4cfe-af68-6c687fd1f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_penguins.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1d2eb-1fd0-4d32-9773-68cda17e1340",
   "metadata": {},
   "source": [
    "Now, `scikit-learn` will not give you p-values and the like, since these are not commonly used in machine learning. But, you can predict the $y$ value of an $X$ observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53d805-372d-4362-bb6a-b39d39d9600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84042f2f-8a43-4ad5-9ca6-66b0fab346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_penguins.predict(X = penguins_X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74bbdc-cf3f-41ba-acbb-379e6ba3744d",
   "metadata": {},
   "source": [
    "Let's compare this to the true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd8bf3-2be5-4bfe-aec8-05b7e95592d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_y.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184184d-f4b4-4f5f-adde-006527949d3a",
   "metadata": {},
   "source": [
    "You can also get the $R^2$ score. You input a whole $X$ matrix on which to do the predictions, together with the true $y$ values, and you will get the error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae617cf-12c3-43a0-9917-09e466453dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_penguins.score(X = penguins_X, y = penguins_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411171b1-ca70-46ab-83be-58b639989a89",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\"> EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eff631-7d73-4b06-ac21-689d5d385d64",
   "metadata": {},
   "source": [
    "1. Create a regression object.\n",
    "2. Fit it to the fish dataset, the target in this case is the weight column.\n",
    "3. Obtain the R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82384b5d-8b2f-484d-acd9-bf9f5f827fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll load the data for you\n",
    "fish_df = pd.read_csv(\"data/fish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362af5d-4e1c-461d-8fa9-6d62ec258282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two distinct dataframes, one for the predictive features and one for the target\n",
    "fish_X = \n",
    "fish_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44481112-51ab-471c-9556-f6d38d798902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cabceb-2481-47ca-9269-b7c4929384ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31160856-ef60-42c8-b2da-a6a0fddbd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will do the preprocessing for you. Feel free to skip this cell and do it yourself.\n",
    "fishX_num = fish_X.drop(columns = [\"species\"])\n",
    "fishX_num = preprocessing.StandardScaler().set_output(transform = \"pandas\").fit_transform(fishX_num)\n",
    "fishX_cat = fish_X[[\"species\"]]\n",
    "fishX_cat = preprocessing.OneHotEncoder(sparse_output = False).set_output(transform = \"pandas\").fit_transform(fishX_cat)\n",
    "\n",
    "fish_X = fishX_num.join(fishX_cat)\n",
    "fish_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7cb51-85cf-442f-b37c-04586b401ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "\n",
    "\n",
    "# Fit to data\n",
    "\n",
    "\n",
    "# Compute R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd7391-8184-4454-9c69-2b4abdbf042e",
   "metadata": {},
   "source": [
    "### <span style = \"color:teal\"> Version 2: The machine learning validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2a203-4305-4128-8b03-2e308d8e3cfc",
   "metadata": {},
   "source": [
    "Mmm... for those of you with more machine learning experience, did something feel off?\n",
    "\n",
    "That's right, we fitted our model to the <b>complete</b> dataset, and also checked its performance based on it. This is actually taboo in machine learning. The reason dates back centuries, and is an important aspect of the philosophy of science. Basically, when we use all our data to create a model, we are \"overfit\" the model, which means it will adapt as much as possible to fit these observations, but at the expense of losing its capacity to generalize to new observations!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fcf272-7dec-41b4-a1bd-3ce8c4bc2851",
   "metadata": {},
   "source": [
    "### <span style = \"color:darkorange\"> Conceptual intermezzo - The bias-variance trade-off, generalization, and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59458089-4e9f-4001-a79d-f3419c8d7ded",
   "metadata": {},
   "source": [
    "See slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23eebf-a483-4e96-b408-3134b3f0be00",
   "metadata": {},
   "source": [
    "<b>So what is the solution?</b>\n",
    "\n",
    "Well, the concensus is to separate the data into <b>training</b>  and <b>testing</b>  data. Then, everything that goes into creating the model must only stem from the training data, while the testing data is kept <span style = \"color:red\"><b>secret</b></span> from the model until the very end. At the end we can test the model on the secret, testing data.\n",
    "\n",
    "<b>How about preprocessing?</b>\n",
    "\n",
    "Some preprocessing steps also use information from the data to estimate parameters (for example the `StandardScaler`, which uses the mean and standard deviation). But, we need to keep the testing data secret from everything used to build the model. Hence, preprocessing should only be fitted using training data. We will see this in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa9faa-47b0-4f10-a215-128453e8caae",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dec0b-bb5d-4798-8b60-0ca0dddb930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data again, to make sure we are working with the correct dataset\n",
    "penguins_df = pd.read_csv(\"data/penguins.csv\")\n",
    "penguins_X = penguins_df[pred_features]\n",
    "penguins_y = penguins_df[[\"body_mass_g\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da7b5c-f912-46bd-9309-9e50aba6a710",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcfdfa-c16f-41db-80d8-364a6921c317",
   "metadata": {},
   "source": [
    "The function `train_test_split()` from the `model_selection` module does this automatically for us. We imported it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d3c60-72f1-44c8-ac5e-37462500e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data:\n",
    "pX_train, pX_test, py_train, py_test = train_test_split(penguins_X, penguins_y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a488f-5737-46e1-9fbf-7ea1ed264e40",
   "metadata": {},
   "source": [
    "The `test_size` parameter is the fraction of the data that will be kept as testing data. Let's check the sizes we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2754f-8193-4748-971c-6b603703d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trainig data: Matrix X of size {pX_train.shape}, target vector y of size {py_train.shape}\\n\" +\n",
    "     f\"Testing data: Matrix X of size {pX_test.shape}, target vector y of size {py_test.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c650c-9f79-49c7-bbec-bc65c72aa0fc",
   "metadata": {},
   "source": [
    "This is how they look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623eef1f-402e-4d6e-87c2-dd14cfce46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7daf2f-f892-4a22-8a70-284b05ed865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd91335-e6f2-4bc4-9468-4087095f2a5c",
   "metadata": {},
   "source": [
    "Did you notice something strange? The indices are all shuffled! Don't panic, as you can see they remain consistent among the feature matrix and the target vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad149bbf-63b7-4be8-83ef-4eab9202eb94",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666e5cf-cbfc-4748-8f4f-9dbf1ccb0f25",
   "metadata": {},
   "source": [
    "We are experienced with this, so we can easily do it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fea304-cb0b-400f-be53-27a24046acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the numerical features\n",
    "sd_scaler = preprocessing.StandardScaler().set_output(transform = \"pandas\")\n",
    "pX_train_num = sd_scaler.fit_transform(pX_train[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e670d4-a608-43bf-99f0-9dce7d7e02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e7d16-6bc2-4d24-8ad4-47871fcf09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the categorical features\n",
    "oh_encoder = preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\").set_output(transform = \"pandas\")\n",
    "pX_train_cat = oh_encoder.fit_transform(pX_train[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7509b-cf18-4c69-ba81-fc2181963b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61d47d-94f3-4d91-869d-7c76bfd9f238",
   "metadata": {},
   "source": [
    "Did you notice my little trick? That's right, since the fit data and the transform data are the same, and transforming after fitting is such a common task, `scikit-learn` provides a method that does both at the same time: `fit_transform`. That saved us a bit of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526f7a8-0f9f-4258-b7d8-0afb6e717f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge them\n",
    "pX_train_all = pX_train_num.join(pX_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90474bef-5ff1-4a6a-bb0f-3ab9f2c562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can take a quick look if you want:\n",
    "pX_train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee49756-a7f6-4e4a-bdde-e8e30df742ef",
   "metadata": {},
   "source": [
    "#### Build the regression model ONLY on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0959eb0-1fdb-41a4-bacd-cc592a912e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_penguins = linear_model.LinearRegression()\n",
    "lm_penguins.fit(X = pX_train_all, y = py_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f32a7-0dff-4c22-9048-0333d9ed06b1",
   "metadata": {},
   "source": [
    "#### The testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac297c-cef7-4d4b-8dd5-456953e8ee52",
   "metadata": {},
   "source": [
    "We also need to preprocess our testing set, BUT, we should do it with the preprocessors that were trained using the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e27a0-1a3c-40fa-927d-115f159747bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale testing data:\n",
    "pX_test_num = sd_scaler.transform(pX_test[num_features])\n",
    "pX_test_cat = oh_encoder.transform(pX_test[cat_features])\n",
    "\n",
    "pX_test_all = pX_test_num.join(pX_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab364fa-8ffc-40a2-9c97-bc9c91fb3d61",
   "metadata": {},
   "source": [
    "Did you notice the difference? When using the training data, we user `fit_transform(X_train)`, this will both fit the preprocessor, and then transform our training data. It is equivalent to first using `.fit()` and then `.transform`.\n",
    "\n",
    "On the contrary, if we are processing the testing data, we should only call `.transform()`, since we don't want to fit the preprocessors with testing data. This would be data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5a2ca-4d3f-408f-b6eb-cb6dd776ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing data:\n",
    "peng_predictions = lm_penguins.predict(pX_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab9641-b543-46f1-9cd6-7d859e116893",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3d82f-8962-4532-a735-cdec9dec18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R2 for testing data:\n",
    "p_r2 = lm_penguins.score(X = pX_test_all, y = py_test)\n",
    "\n",
    "print(f\"The coefficient of determination R2 is {p_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e2753-ea8d-4398-8a88-3fd3be60c090",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3781e-87fe-4b00-97f4-5abdbf319308",
   "metadata": {},
   "source": [
    "Did you see how easy everything became once we understood the different `scikit-learn` classes? We just needed a few lines!! Indeed, here is the code again, without all those mid-code checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4abe9f-8a1a-48c2-8450-a6783fd7f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "penguins_df = pd.read_csv(\"data/penguins.csv\")\n",
    "pX_train, pX_test, py_train, py_test = train_test_split(penguins_df[pred_features], penguins_y, test_size = .2)\n",
    "\n",
    "# -- Training stage --\n",
    "\n",
    "# Preprocess training data\n",
    "sd_scaler = preprocessing.StandardScaler().set_output(transform = \"pandas\")\n",
    "pX_train_num = sd_scaler.fit_transform(pX_train[num_features])\n",
    "\n",
    "oh_encoder = preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\").set_output(transform = \"pandas\")\n",
    "pX_train_cat = oh_encoder.fit_transform(pX_train[cat_features])\n",
    "\n",
    "pX_train_all = pX_train_num.join(pX_train_cat)\n",
    "\n",
    "# Make and fit model\n",
    "lm_penguins = linear_model.LinearRegression().fit(X = pX_train_all, y = py_train)\n",
    "\n",
    "# -- Testing stage --\n",
    "\n",
    "# Process testing data:\n",
    "pX_test_num = sd_scaler.transform(pX_test[num_features])\n",
    "pX_test_cat = oh_encoder.transform(pX_test[cat_features])\n",
    "pX_test_all = pX_test_num.join(pX_test_cat)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"The coefficients are: {lm_penguins.coef_}\")\n",
    "print(f\"The R2 score is: {lm_penguins.score(X = pX_test_all, y = py_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b95ca6-767c-4acc-831d-25b2b303f334",
   "metadata": {},
   "source": [
    "## <span style = \"color:red\"> Long Exercise - Bo's Fishy Quest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4d74e-616c-42ee-8715-78484380cbaf",
   "metadata": {},
   "source": [
    "Now it's Bo's time to shine!\n",
    "\n",
    "Alice asks Bo for help with the penguin project. This time, they want to be able to take images of fish at a large scale, and estimate their weight based on visual features. This will allow them to keep track of food availability for the penguins, and of the ecosystem health in general. As with the penguins, the visual features will be extracted using some fancy computer vision software.\n",
    "\n",
    "Bo already has this dataset, which is the fish dataset you've been working on. Your task is to create a linear regression model on this dataset. Remember that the $y$ values, or target, are the weights of the fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9d6a4-241e-4a54-bb9a-aaeb5bad10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the fish dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09fd86-c21e-44e0-80e1-2ba14ae469e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Specify predictive features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85108b5e-764e-4979-ba6b-f5125da602b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split training and testing data. You can choose your  test size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd008c-17b2-4434-a44d-d7b516d8c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Preprocess the training data. Keep preprocessors for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac49c6-4861-4334-a1c7-2c0dab9501f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make and fit the linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52750e-1360-4948-96d1-b00e90b77f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Preprocess the testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d7013-6005-479e-8b0d-f25b5b43313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a690140-9331-41ed-a2e8-427c35d482e8",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\">Preprocessing revisited: the ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454df4c1-c817-4bb7-8c39-8514f9619f69",
   "metadata": {},
   "source": [
    "Before we continute to pipelines, let's learn a new trick. Was it not super annoying that when it came to preprocessing, we had to do the numeric variables and the categorical ones separately and then join them? Some of you may want to avoid this by preprocessing your data outside of `scikit-learn`, but there are important advantages to do it within it (avoid data leakage and do parameter search, for example).\n",
    "\n",
    "To ease our existential burdens, we have the `ColumnTransformer` from the `compose` module, which we have imported already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dbedc-c87e-4406-8b09-9cd09f51bd05",
   "metadata": {},
   "source": [
    "The way it works is straightforward, we create the object and pass it a list of tuples. These tuples contain three things: the name we want to call each of our *transformers* (like our preprocessor objects), the objects themselves, and the column of our data over which we want the transformer to act.\n",
    "\n",
    "This makes more sense in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6400a8f-64c7-4474-9ce1-0c55d27c445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd84eda-a711-4fbd-beb6-e59cded8fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer object\n",
    "col_trans = ColumnTransformer(\n",
    "    [(\"cat\", preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\"), cat_features),\n",
    "    (\"num\", preprocessing.StandardScaler(), num_features)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507df31-83ae-4b65-a6a8-bc7cfe1575d0",
   "metadata": {},
   "source": [
    "Guess what, we can set the output format for column transformers also, and we just need to do it once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95fbb6-edba-4377-8945-88f7ffd6c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output to pandas format for all transformers at the same time\n",
    "col_trans.set_output(transform = \"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1348bf-ae55-4277-abd3-3c629325d610",
   "metadata": {},
   "source": [
    "Woah, what just happened here? Well, it's showing us a nice diagram of our column transformer, click on the arrows to see what's inside. We will see more of these diagrams soon.\n",
    "\n",
    "Do you see the line exiting the bottom? It's indicating to us what the output is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0b1e6-b71d-4cce-bdc3-00703c0a2227",
   "metadata": {},
   "source": [
    "We can also create the column transformer and set the output data in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9ad9b-dc8c-4bee-b43d-f8ab7f2b2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_trans = ColumnTransformer(\n",
    "    [(\"cat\", preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\"), cat_features),\n",
    "    (\"num\", preprocessing.StandardScaler(), num_features)]\n",
    ").set_output(transform = \"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa97b5c-8339-4ca1-8134-143008248860",
   "metadata": {},
   "source": [
    "Now, we haven't given it any data yet. So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c082333-7bae-44ba-935d-6664d9fccf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_transformed = col_trans.fit_transform(penguins_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbccc1e-674a-4e58-be44-8ca148b2c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d4a9a-3d71-404b-b784-f74ffcfef098",
   "metadata": {},
   "source": [
    "Is this cool or what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa701e3-5c41-443e-b6fa-42ea8ed6b218",
   "metadata": {},
   "source": [
    "# <span style = \"color:rebeccapurple\"> Part 3 - The Pipeline class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c30f83-247a-4c32-9a6a-a456f5654635",
   "metadata": {},
   "source": [
    "OK, it is time to talk about pipelines. A pipeline is an **abstraction** representing the whole process through which we implement a machine learning solution. A simplified pipeline may look like this:\n",
    "\n",
    "1. State the problem.\n",
    "2. Gather data.\n",
    "3. Split training and testing data.\n",
    "4. Preprocess the data.\n",
    "5. Train the model.\n",
    "6. Evaluate and optimize the model.\n",
    "7. Draw awesome figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dba02-79c1-4c42-8eb8-fb89d8c9b178",
   "metadata": {},
   "source": [
    "Having such a guide is very useful, and a starting point. However, 1) not all machine learning tasks follow the recipe above, so we must remain flexible, and 2) no realistic process is linear.\n",
    "\n",
    "A realistic process would be more of a dynamic web of relationships. However, this model will do for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b5178-e7a9-4531-b435-324666dceb60",
   "metadata": {},
   "source": [
    "As it turns out, `scikit-learn` makes our life even easier by letting us specify our own pipeline. This is done through the `Pipeline` class, which we imported already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ca4ed-0af7-4f10-a821-07dcbad0e4db",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\"> Our first pipeline object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa60c2-e0c3-4c0e-a97b-d9f5285d894a",
   "metadata": {},
   "source": [
    "Each pipeline object requires a list of *steps* in the pipeline, which we specify as a list of tuples. The tuples consist of two elements: the name we want to give a given step (for example, \"scaler\", \"regressor\", etc.), and then the actual object that will realize that step of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2162394-ab44-4c24-ad5c-1e1cb9a3b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of steps:\n",
    "pipe_list = [(\"step1_sd_scaler\", preprocessing.StandardScaler()),\n",
    "            (\"step2_lm\", linear_model.LinearRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655e676-af4c-453e-be2e-55a4fd598271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "my_pipe = Pipeline(pipe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7543e-1281-4db1-b847-784b123ea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f558c58-27c4-4cc6-a694-8d0ed2a5be77",
   "metadata": {},
   "source": [
    "Nice! That right there is a pipeline object. Looks a bit like our `ColumnTransformer` right? Notice the output is the output of the LinearRegression object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c21990-c58b-4546-a1a0-2f9d89f2edbd",
   "metadata": {},
   "source": [
    "What's great about this is we can now just fit the whole pipeline to our dataset and it will magically work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1294fa-0495-4287-886a-0ea5f5b21854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some dummy data\n",
    "penguins_X_num = pd.read_csv(\"data/penguins.csv\")[num_features]\n",
    "penguins_y = pd.read_csv(\"data/penguins.csv\")[\"body_mass_g\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79f73e-2ad3-4f6f-9792-92de5b32aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline:\n",
    "my_pipe.fit(penguins_X_num, penguins_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb267d-70c9-48b4-9290-42c43ae14738",
   "metadata": {},
   "source": [
    "So what did it do? It first scaled our data, and then it took the output of that data and used it to fit a linear regression model.\n",
    "\n",
    "The pipeline can calculate your $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd4b62-3b9b-4140-9d4c-150333492874",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe.score(penguins_X_num, penguins_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2dbffd-f821-4662-ac59-93799ba8415b",
   "metadata": {},
   "source": [
    "But it can't get you the regression coefficients directly. For that, you'll need to access the linear regression object. You can access each step with python indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c747467-681c-42c3-91ab-d5e9df2e37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access first step\n",
    "my_pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1bd22-61a2-4f59-88bf-202707251d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access second step\n",
    "my_pipe[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a3e96-d759-440b-8dc8-b8111cfbf5ea",
   "metadata": {},
   "source": [
    "Now we can get the regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ee1d0-2d02-471a-8d17-a925e49013bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a68feb-0747-47b3-8468-332c01b0c03d",
   "metadata": {},
   "source": [
    "We can also access the steps by name, as in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0037b47-1aa7-473d-ab10-ac008950e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access linear regression object, and get coefficients from there.\n",
    "my_pipe[\"step2_lm\"].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d9330-1506-42b5-9620-5038db4bf071",
   "metadata": {},
   "source": [
    "### <span style = \"color:teal\"> Don't forget about the train test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7d2b1-0c2d-4974-a2ba-2cb5ce71bac4",
   "metadata": {},
   "source": [
    "OK, the above helped us get familiar with pipelines, but we didn't split our data. That's a sin! Let's atone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c628e-c489-4885-88bb-539723d9e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "penguins_X_num = pd.read_csv(\"data/penguins.csv\")[num_features]\n",
    "penguins_y = pd.read_csv(\"data/penguins.csv\")[\"body_mass_g\"]\n",
    "\n",
    "# Split data right away:\n",
    "pX_train, pX_test, py_train, py_test = train_test_split(penguins_X_num, penguins_y, test_size = .3)\n",
    "\n",
    "# Create pipeline:\n",
    "penguins_pipeline = Pipeline([(\"step1_sd_scaler\", preprocessing.StandardScaler()),\n",
    "            (\"step2_lm\", linear_model.LinearRegression())])\n",
    "\n",
    "# Fit pipeline:\n",
    "penguins_pipeline.fit(pX_train, py_train)\n",
    "\n",
    "# Get score on test dataset:\n",
    "penguins_pipeline.score(pX_test, py_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97083f29-5550-4b63-ba16-2b912467958b",
   "metadata": {},
   "source": [
    "Now hold on a sec, didn't we have to preprocess the testing data also? Don't worry, the pipeline does it for us!\n",
    "\n",
    "When we use the `fit()` method, the entire pipeline is fitted with the given data. However, when we use methods like `score()` and `predict()`, it uses the already fitted values to preprocess the given data, and then it performs the scoring or prediction on the preprocessed data. How cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc762b-4ee6-44fe-b99d-b14b26662e7f",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\"> Implementing ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0915121-4d72-41b3-8d75-ea1a4a1b8a92",
   "metadata": {},
   "source": [
    "Above we only used the numerical data in our pipeline. But what if we also have categorical data, and we want a one hot encoder together with a standard scaler? Well, no sweat, we can take our `ColumnTransformer()` apraoch above, and use it with our pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c1b73-902b-41cd-b610-631e35d0ec77",
   "metadata": {},
   "source": [
    "<b> Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc40655-f9db-43cc-86f4-b3b9b1072324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some dummy data\n",
    "penguins_X = pd.read_csv(\"data/penguins.csv\")[pred_features]    # <-- Using all predictive features\n",
    "penguins_y = pd.read_csv(\"data/penguins.csv\")[[\"body_mass_g\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043377d-2070-4413-a144-5c2f65e32baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fa8fc-d235-4104-baa8-7c45fe524e39",
   "metadata": {},
   "source": [
    "<b> Split the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf89bff-108b-44a3-b5d7-5d31a1ee9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train, pX_test, py_train, py_test = train_test_split(penguins_X, penguins_y, test_size = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b3222-7a32-47e5-9152-3b19c7d3d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a5adb-3918-4345-9673-2ccb9f7b08c3",
   "metadata": {},
   "source": [
    "<b> Create a ColumnTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ce03a-2c18-4402-9485-542ed9a1b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer\n",
    "col_trans = ColumnTransformer(\n",
    "    [(\"cat\", preprocessing.OneHotEncoder(sparse_output = False, drop = \"if_binary\"), cat_features),\n",
    "    (\"num\", preprocessing.StandardScaler(), num_features)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73a155-8a57-457c-8372-50d677465e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pipeline\n",
    "penguins_pipeline = Pipeline([\n",
    "    (\"col_trans\", col_trans),\n",
    "    (\"linear_model\", linear_model.LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad3408-fb59-4464-a22b-cd21010ca33d",
   "metadata": {},
   "source": [
    "<b> Now we fit and score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c070fd-e4dd-467e-9fcf-ea6d16132150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline\n",
    "penguins_pipeline.fit(pX_train, py_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d408b25-5db3-4909-ab69-25a568b0c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Score:\n",
    "penguins_pipeline.score(pX_test, py_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc00015-5aba-4243-a282-de258c803183",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce9a81-52de-471c-a9d6-e6f2ecf237ef",
   "metadata": {},
   "source": [
    "Just as we can make a column tranformer a part of a pipeline, you can make a pipeline a part of a column transformer. For example if each column requires several steps of preprocessing, before they merge, you would create a preprocessing pipeline for each of them, then join them with a column transformer, then embed that column transformer into a larger pipeline.\n",
    "\n",
    "We don't have time to do this but it's useful info for the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61cf0b4-828e-43e3-9fa5-1487cd1fe3bf",
   "metadata": {},
   "source": [
    "## <span style = \"color:darkorchid\"> Alice in Antarctica - Redux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead90f16-bc11-4e64-88c1-c6eaa0d741d4",
   "metadata": {},
   "source": [
    "Time to put everything we've done together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c3c22-b846-4359-a8b8-146ecfda8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load the data\n",
    "penguins_X = pd.read_csv(\"data/penguins.csv\")[pred_features]\n",
    "penguins_y = pd.read_csv(\"data/penguins.csv\")[[\"body_mass_g\"]]\n",
    "\n",
    "# Step 2 - Split the data\n",
    "pX_train, pX_test, py_train, py_test = train_test_split(penguins_X, penguins_y, test_size = .3)\n",
    "\n",
    "# Step 3 - Create transformers and pipelines\n",
    "col_trans = ColumnTransformer(\n",
    "    [(\"cat\", preprocessing.OneHotEncoder(drop = \"if_binary\"), cat_features),\n",
    "    (\"num\", preprocessing.StandardScaler(), num_features)])\n",
    "\n",
    "penguins_pipeline = Pipeline([\n",
    "    (\"col_trans\", col_trans),\n",
    "    (\"linear_model\", linear_model.LinearRegression())\n",
    "])\n",
    "\n",
    "# Step 4 - Fit full pipeline\n",
    "penguins_pipeline.fit(pX_train, py_train)\n",
    "\n",
    "# Step 5 - Evaluate\n",
    "penguins_pipeline.score(pX_test, py_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1770c-471c-4ab3-8707-25352eb6405b",
   "metadata": {},
   "source": [
    "Notice that we keep \"abstracting away\", and taking a perspective at higher and higher levels. This is the art of good object oriented programming, and also of a complicated machine learning / data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11638b7a-fd25-4fd6-a240-cd84e3023293",
   "metadata": {},
   "source": [
    "## <span style = \"color:red\"> Bob's fishy quest - Redux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d95cd-ecc2-45af-8e66-ef250faec106",
   "metadata": {},
   "source": [
    "You know what to do here ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689251e-dd67-4dc3-93b4-9d0fa2dd92a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1eb05ca-c91e-45ec-b6f4-fd6e44786e60",
   "metadata": {},
   "source": [
    "# <span style = \"color:darkorange\"> Conceptual Intermezzo - Machine Learning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254da066-af60-4afa-bd40-77178c71bb93",
   "metadata": {},
   "source": [
    "See slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e92b6-d20f-4c36-b6e4-fc674674b97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "panel-cell-order": [
   "9cc01510-630f-42c8-9112-4da50a57ab6a",
   "5b082360-237e-4ba3-a055-c6ab1f45abb5",
   "5bdc2be4-c689-451a-99eb-bcdd9ef6c8f5",
   "e2e1e398-a0d4-4253-a250-7af6dd620528",
   "85353d77-3bc8-4cfa-867f-1adc24765050",
   "909fbd52-47ae-4dbf-8e81-8e656029e1e2",
   "d76fb7bc-fce8-41b8-b5ac-21d69c2fa660",
   "a77f401e-c06c-47ff-b401-c428b6561fda",
   "a380bf25-42e7-4db8-89f3-b9f395a009a3",
   "1b2b0957-8ae6-488c-82aa-05e3c704acbf",
   "2eb29f43-3975-48c4-8dfd-0f8c6957719a",
   "48095baf-bd2d-4860-b83b-238479c55e43",
   "e3101a01-d755-4c90-8037-b094d273ad60",
   "aa4adf62-b240-49ac-b057-4f475dbb2061",
   "5de79fc5-7d21-4711-a8e5-30edf1e430de",
   "48359de5-eea5-44fd-b148-4950ed66d90b",
   "17037863-9ef6-4a5d-a842-33d3d1069186",
   "64f84854-086f-471e-ac9a-996074de6b63",
   "d2d95da1-15a4-4f76-acb7-3a1081c3f862",
   "6bf86c16-2a91-4401-bb67-4f693996e86a",
   "0fde2a73-03d6-41c5-a6f1-53b274f6f538",
   "e20c8781-cf90-4b44-bd88-c99b933227ab",
   "e31700c5-4d2f-4d7a-ae72-4fce0e093b0a",
   "ec0e52b3-ad02-4f0c-9569-0f031d299b9c",
   "ce73879b-97da-483c-b754-3c89b731d5a3",
   "43d9c52a-efdd-4903-9966-3644788b7c52",
   "ef14c206-cdf3-4fd9-a0d4-dfd5d16ce410",
   "d5a2c1df-34b8-4c28-9f64-ee803421f3d5",
   "191905e0-0a6b-484e-948d-84f3387c87cb",
   "ce1c111d-d4aa-4c08-9d88-87c4b52d1d41",
   "ebd739a5-ae8c-41e4-b87b-a1d28401ee1e",
   "5d2fe31b-3d23-46eb-88dc-0d330b217461",
   "7e8c6446-0727-4548-8a92-4bdf543ff508",
   "75097743-a5c5-411d-9f53-ab0396b55544",
   "853cde47-e945-49f4-87d5-1e1834afdc59",
   "74a013d0-9471-444c-a078-0aaf46d5f372",
   "fb6c5094-edab-479b-bbae-0908d77b29ed",
   "abd6b0a3-bf39-431d-a592-166c73e8b7ce",
   "34c89974-8212-4605-aa2f-5c40d50d3719"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
